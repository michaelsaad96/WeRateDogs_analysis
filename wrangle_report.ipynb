{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Data Wrangling for WeRateDogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report shows the wrangling efforts for the data from WeRateDogs twitter account. This data was first gathered from three different sources, assessed visually and programatically, and then cleaned so that it is more useful for gaining insights and making visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We read `twitter_archive_enhanced.csv` file into a dataframe. This file contains the tweet data from WeRateDogs account, icluding the tweet text, timestamp, and expanded URL.\n",
    "2. We downloaded the image predictions file provided by the course using the `get` method from the `requests` library. Then, we used the `StringIO` library to convert the contents of the response into a file like object, which we then read into a dataframe using pandas `read_csv` function.\n",
    "3. We used the Twitter API to query for more data on each tweet in our dataset using its tweet id. We stored the results in a Json file whic we then read into a dataframe using pandas `read_json` function. We were only interested in the `id`, `retweet_count`, and `favorite_count` in this dataset, so we dropped the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a number of issues while examining the dataset. These issues can be subdivided into quality and tidiness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issues\n",
    "#### General\n",
    "* Completeness: the number of entries in the three dataframes `image_predictions`, `twitter_archive`, and `twitter_stats` wasn't the same.\n",
    "\n",
    "#### `twitter_archive` table\n",
    "* The `timestamp` column had wrong dtype (`object`). It should be (`datetime`).\n",
    "* Some ratings were extracted incorrectly (for example 5/10 istead of 13.5/10).\n",
    "* Sometimes the wrong value was extracted as a rating (for example dates like 9/11).\n",
    "* Some ratings were for groups of dogs rather than one dog. Those had a denominator that was 10x the number of dogs.\n",
    "* Some rows contained retweets and replies, which we aren't interested in as they aren't dog ratings.\n",
    "* Some rows contained empty values in the `expanded_urls` column.\n",
    "* The `expanded_url` column sometimes contained more than one url.\n",
    "* Some rows contained invalid values (links that weren't twitter photos) in the `expanded_urls` column.\n",
    "* Some rows contained invalid values for the `name` column (such as 'a', 'by', and 'the').\n",
    "\n",
    "#### `image_predictions` table\n",
    "* The `p1`, `p2`, and `p3` columns had wrong dtype (`object`). They should be of dtype (`category`).\n",
    "\n",
    "### Tidiness Issues\n",
    "* Data for dogs and tweets should be divided into seperate dataframes.\n",
    "\n",
    "#### `twitter_archive` table\n",
    "* The `doggo`, `floofer`, `pupper`, and `puppo` columns were values rather than variables.\n",
    "\n",
    "#### `image_predictions` table\n",
    "* `p1`, `p2`, and `p3` were the same variable present in three different columns. The same goes for `p1_conf`, `p2_conf`, and `p3_conf` as well as `p1_dog`, `p2_dog`, and `p3_dog`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes how the issues found above were addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `image_predictions`: the `p1`, `p2`, and `p3` columns had wrong dtype (`object`). They should be of dtype (`category`).\n",
    "#### `twitter_archive`: the `timestamp` column had wrong dtype (`object`). It should be (`datetime`).\n",
    "\n",
    "##### Solution\n",
    "We changed the incorrect column datatypes into the appropriate ones using pandas `astype` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `twitter_archive`: some ratings were extracted incorrectly (for example 5/10 istead of 13.5/10).\n",
    "#### `twitter_archive`: sometimes the wrong value was extracted as a rating (for example dates like 9/11).\n",
    "\n",
    "##### Solution\n",
    "* To solve that ratings with fractional values were not extracted correctly, we wrote a new regex to extract whole-number as well as fractional-number ratings.\n",
    "* To solve that sometimes an incorrect value was extracted as a rating, we observed that the correct rating was always at the end of the tweet. That's why we decided to use the last match. The maximum number of matches found was 3. We extracted all gruops that matched the regex, and used the last one as our rating.\n",
    "* Some of the extracted values were still not dog ratings (e.g. 24/7), so we dropped the values whose denominator was indivisible by 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `twitter_archive`: some ratings were for groups of dogs rather than one dog. Those had a denominator that is 10x the number of dogs.\n",
    "\n",
    "##### Solution\n",
    "We recalculated all the rating numerators to be out of 10, then we set all the denominators to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `twitter_archive`: some rows contained retweets and replies, which we weren't interested in as they aren't dog ratings.\n",
    "\n",
    "##### Solution\n",
    "We identified that retweets had non-empty values in `retweeted_status_id` column, so we dropped those rows. Same goes for replies and `in_reply_to_status_id` column. Afterwards we dropped all columns related to retweets and replies as they were not needed anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `twitter_archive`: some rows contained empty values in the `expanded_urls` column.\n",
    "\n",
    "##### Solution\n",
    "Some tweets didn't contain photos, which resulted in those rows having empty values in `expanded_urls` column. Those were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `twitter_archive`: some rows contained invalid values for the `name` column (such as 'a', 'by', and 'the').\n",
    "\n",
    "##### Solution\n",
    "* We noticed that all valid dog names begin with capital letters, so we replaced the names that begins with small letters with `np.NaN`.\n",
    "* We replaced `None` with `np.NaN`.\n",
    "* We manually replaced `O` with `O'Malley`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `twitter_archive`: the `doggo`, `floofer`, `pupper`, and `puppo` columns were values rather than variables.\n",
    "\n",
    "##### Solution\n",
    "To make the dataframe more tidy, we decided to replace the four value columns with one variable column called `dog_rank`. It was observed that some rows had `None` in all four columns, while others had one or more columns which weren't `None`. Using `melt()` directly to transform the four columns into rows would leave us unable to correctly identify and drop duplicates. That's why we first assigned a temporary rank of `doge` to rows who didn't have a rank, used `melt()`, and then dropped rows which had `None` in the `dog_rank` column. Finally, we replaced the temorary rank `doge` with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `image_predictions`: `p1`, `p2`, and `p3` were the same variable present in three different columns. \n",
    "#### The same goes for `p1_conf`, `p2_conf`, and `p3_conf` as well as `p1_dog`, `p2_dog`, and `p3_dog`.\n",
    "\n",
    "##### Solution\n",
    "We made a temporary dataframe for each prediction of the three, then we concatenated them into one big dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for dogs and tweets should be divided into seperate dataframes.\n",
    "\n",
    "##### Solution\n",
    "Since tidiness guidelines require that each unit of observation be a separate dataframe, we decided to have one dataframe for tweets and another for dogs.\n",
    "\n",
    "* We merged `twitter_stats` dataframe onto `twitter_archive`.\n",
    "* We made a new dataframe `dog_table` by merging `tweet_id`, `rating_numerator`, `rating_denominator`, `name`, and `dog_rank` columns from `twitter_archive` onto `image_predictions`.\n",
    "* We dropped `rating_numerator`, `rating_denominator`, `name`, `dog_rank` columns from the tweets dataframe so that we don't have duplicate columns in multiple dataframes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
